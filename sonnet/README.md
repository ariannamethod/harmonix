**Try it live here:**  
ğŸ‘‰ [https://huggingface.co/spaces/alzaemaliq/NanoGPT-Shakespeare](https://huggingface.co/spaces/alzaemaliq/NanoGPT-Shakespeare)

# NanoGPT-Shakespeare

This is a basic reimplementation of the "Attention Is All You Need" paper, focused on the decoder-only transformer architecture used in GPT models. The model is trained from scratch on the Tiny Shakespeare dataset using character-level tokenization.

The code is written for readability and learning. It avoids high-level libraries or frameworks and builds the model components (multi-head attention, feedforward layers, positional embeddings, etc.) using raw PyTorch.

## Features

- Decoder-only transformer (GPT-style)
- Character-level tokenizer (custom encode/decode logic)
- Multi-head self-attention with causal masking
- Learned positional embeddings
- LayerNorm and residual connections
- Simple training loop using AdamW
- Generates text one character at a time

## Model details

- Embedding dimension: 128
- Number of heads: 4
- Number of layers: 4
- Context window (block size): 64
- Batch size: 16
- Dropout: 0.1
- Training steps: 9000

## Data

The model is trained on the Tiny Shakespeare dataset (a small corpus of Shakespeare text). Character-level tokenization is used instead of word or subword tokenization.

## Notes

The `train.py` file contains the full model implementation and training loop. It also includes many inline comments and side notes where I recorded my thoughts while learning and building the model. Some of these comments may be messy or unintelligible at times, as they were written in the moment during the learning process.

**ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¢ã¯ã“ã¡ã‚‰ï¼š**  
ğŸ‘‰ [https://huggingface.co/spaces/alzaemaliq/NanoGPT-Shakespeare](https://huggingface.co/spaces/alzaemaliq/NanoGPT-Shakespeare)

# NanoGPT-Shakespeare

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ŒAttention Is All You Needã€è«–æ–‡ã®åŸºæœ¬çš„ãªå†å®Ÿè£…ã§ã‚ã‚Šã€GPTãƒ¢ãƒ‡ãƒ«ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ€å°‚ç”¨ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚  
Tiny Shakespeareãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€æ–‡å­—å˜ä½ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

ã‚³ãƒ¼ãƒ‰ã¯å­¦ç¿’ã¨å¯èª­æ€§ã‚’é‡è¦–ã—ã¦ãŠã‚Šã€é«˜ãƒ¬ãƒ™ãƒ«ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¿ã‘ã€ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ãªã©ã®ãƒ¢ãƒ‡ãƒ«æ§‹æˆè¦ç´ ã‚’PyTorchã§ä½ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

## ç‰¹å¾´

- ãƒ‡ã‚³ãƒ¼ãƒ€å°‚ç”¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆGPTã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
- æ–‡å­—å˜ä½ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆç‹¬è‡ªã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
- ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è‡ªå·±æ³¨æ„æ©Ÿæ§‹ï¼ˆå› æœãƒã‚¹ã‚­ãƒ³ã‚°ä»˜ãï¼‰
- å­¦ç¿’ã•ã‚ŒãŸä½ç½®åŸ‹ã‚è¾¼ã¿
- LayerNormã¨æ®‹å·®æ¥ç¶š
- AdamWã«ã‚ˆã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªå­¦ç¿’ãƒ«ãƒ¼ãƒ—
- ãƒ†ã‚­ã‚¹ãƒˆã‚’1æ–‡å­—ãšã¤ç”Ÿæˆ

## ãƒ¢ãƒ‡ãƒ«è©³ç´°

- åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ï¼š128  
- ãƒ˜ãƒƒãƒ‰æ•°ï¼š4  
- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼š4  
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºï¼‰ï¼š64  
- ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼š16  
- ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆï¼š0.1  
- å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼š9000

## ãƒ‡ãƒ¼ã‚¿

Tiny Shakespeareï¼ˆã‚·ã‚§ã‚¤ã‚¯ã‚¹ãƒ”ã‚¢ã®å°è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼‰ã‚’ä½¿ç”¨ã€‚  
å˜èªã‚„ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã§ã¯ãªãã€æ–‡å­—å˜ä½ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

## è£œè¶³

`train.py` ã«ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ—ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚  
å¤šãã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ¡ãƒ¢ãŒã‚ã‚Šã€å­¦ç¿’ä¸­ã«æ€ã„ã¤ã„ãŸã“ã¨ã‚’ãã®ã¾ã¾æ›¸ãç•™ã‚ã¦ã„ã¾ã™ã€‚  
ãã®ãŸã‚ã€é›‘ã ã£ãŸã‚Šèª­ã¿ã¥ã‚‰ã„éƒ¨åˆ†ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã”äº†æ‰¿ãã ã•ã„ã€‚
